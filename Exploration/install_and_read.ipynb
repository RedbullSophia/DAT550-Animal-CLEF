{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the WildlifeReID-10k dataset\n",
    "\n",
    "This notebook show a basic analysis of the [WildlifeReID-10k](https://www.kaggle.com/datasets/wildlifedatasets/wildlifereid-10k) dataset. We first analyze the dataset and then show to evaluate an already trained model. We first import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T09:18:12.771714Z",
     "iopub.status.busy": "2025-03-11T09:18:12.771368Z",
     "iopub.status.idle": "2025-03-11T09:19:31.324662Z",
     "shell.execute_reply": "2025-03-11T09:19:31.3234Z",
     "shell.execute_reply.started": "2025-03-11T09:18:12.77169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Hub\\DAT550-Animal-CLEF\\Exploration\n",
      "Collecting git+https://github.com/WildlifeDatasets/wildlife-datasets@develop\n",
      "  Cloning https://github.com/WildlifeDatasets/wildlife-datasets (to revision develop) to c:\\users\\trade\\appdata\\local\\temp\\pip-req-build-hek_ycds\n",
      "  Resolved https://github.com/WildlifeDatasets/wildlife-datasets to commit 959c6c01a8317ed4f162ebfdf4b5e63faad60228\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting git+https://github.com/WildlifeDatasets/wildlife-tools\n",
      "  Cloning https://github.com/WildlifeDatasets/wildlife-tools to c:\\users\\trade\\appdata\\local\\temp\\pip-req-build-fs537d76\n",
      "  Resolved https://github.com/WildlifeDatasets/wildlife-tools to commit 71aa4656d16afe4caae6d84af642bab81dc2d06d\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (2.2.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (4.67.1)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.62 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (4.11.0.86)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (11.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (1.6.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (3.10.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (2.0.8)\n",
      "Collecting datasets (from wildlife-datasets==1.0.5)\n",
      "  Using cached datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: gdown in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (5.2.0)\n",
      "Requirement already satisfied: kaggle in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-datasets==1.0.5) (1.7.4.2)\n",
      "Collecting gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c (from wildlife-tools==1.0.1)\n",
      "  Using cached gluefactory-0.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch>=2.0.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-tools==1.0.1) (2.6.0)\n",
      "Requirement already satisfied: timm>=0.9.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-tools==1.0.1) (1.0.15)\n",
      "Requirement already satisfied: tensorboard in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-tools==1.0.1) (2.19.0)\n",
      "Requirement already satisfied: pytorch_metric_learning in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-tools==1.0.1) (2.8.1)\n",
      "Requirement already satisfied: transformers>=4.30.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-tools==1.0.1) (4.50.2)\n",
      "Requirement already satisfied: kornia>=0.6.12 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from wildlife-tools==1.0.1) (0.8.0)\n",
      "Requirement already satisfied: kornia_rs>=0.1.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kornia>=0.6.12->wildlife-tools==1.0.1) (0.1.8)\n",
      "Requirement already satisfied: packaging in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kornia>=0.6.12->wildlife-tools==1.0.1) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets==1.0.5) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets==1.0.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets==1.0.5) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets==1.0.5) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets==1.0.5) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets==1.0.5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from pandas>=1.1.4->wildlife-datasets==1.0.5) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from pandas>=1.1.4->wildlife-datasets==1.0.5) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets==1.0.5) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets==1.0.5) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets==1.0.5) (3.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from timm>=0.9.2->wildlife-tools==1.0.1) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from timm>=0.9.2->wildlife-tools==1.0.1) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from timm>=0.9.2->wildlife-tools==1.0.1) (0.29.3)\n",
      "Requirement already satisfied: safetensors in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from timm>=0.9.2->wildlife-tools==1.0.1) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from torch>=2.0.1->wildlife-tools==1.0.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.1->wildlife-tools==1.0.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from tqdm>=4.62.3->wildlife-datasets==1.0.5) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from transformers>=4.30.2->wildlife-tools==1.0.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from transformers>=4.30.2->wildlife-tools==1.0.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from transformers>=4.30.2->wildlife-tools==1.0.1) (0.21.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from datasets->wildlife-datasets==1.0.5) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from datasets->wildlife-datasets==1.0.5) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from datasets->wildlife-datasets==1.0.5) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from datasets->wildlife-datasets==1.0.5) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from datasets->wildlife-datasets==1.0.5) (3.11.14)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from gdown->wildlife-datasets==1.0.5) (4.13.3)\n",
      "Collecting lightglue@ git+https://github.com/cvg/LightGlue.git (from gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1)\n",
      "  Cloning https://github.com/cvg/LightGlue.git to c:\\users\\trade\\appdata\\local\\temp\\pip-install-cla5r4lq\\lightglue_b77de58a12fa4c948e988847e28e617c\n",
      "  Resolved https://github.com/cvg/LightGlue.git to commit edb2b838efb2ecfe3f88097c5fad9887d95aedad\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: h5py in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (3.13.0)\n",
      "Requirement already satisfied: omegaconf in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (2.3.0)\n",
      "Requirement already satisfied: albumentations in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (2.0.5)\n",
      "Requirement already satisfied: seaborn in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (0.13.2)\n",
      "Requirement already satisfied: bleach in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (3.4.1)\n",
      "Requirement already satisfied: idna in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (6.30.2)\n",
      "Requirement already satisfied: python-slugify in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (8.0.4)\n",
      "Requirement already satisfied: six>=1.10 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (2.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from kaggle->wildlife-datasets==1.0.5) (0.5.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from tensorboard->wildlife-tools==1.0.1) (2.2.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from tensorboard->wildlife-tools==1.0.1) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from tensorboard->wildlife-tools==1.0.1) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from tensorboard->wildlife-tools==1.0.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from tensorboard->wildlife-tools==1.0.1) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from aiohttp->datasets->wildlife-datasets==1.0.5) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->wildlife-tools==1.0.1) (3.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (2.10.6)\n",
      "Requirement already satisfied: albucore==0.0.23 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from albucore==0.0.23->albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (3.12.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from albucore==0.0.23->albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (6.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from beautifulsoup4->gdown->wildlife-datasets==1.0.5) (2.6)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from omegaconf->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (4.9.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets==1.0.5) (1.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\hub\\dat550-animal-clef\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations->gluefactory@ git+https://github.com/cvg/glue-factory.git@1f56839db2242929960d70f85bfac6c19ef2821c->wildlife-tools==1.0.1) (2.27.2)\n",
      "Using cached datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Building wheels for collected packages: wildlife-datasets, wildlife-tools, lightglue\n",
      "  Building wheel for wildlife-datasets (pyproject.toml): started\n",
      "  Building wheel for wildlife-datasets (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wildlife-datasets: filename=wildlife_datasets-1.0.5-py3-none-any.whl size=88910 sha256=cef86471de935d117de4f528704d648074327c7a74dd7e8593028b8edd8b47f0\n",
      "  Stored in directory: C:\\Users\\trade\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7awgsrz7\\wheels\\2d\\84\\b9\\40d8ea38f41340a93cbe0d60f5b88eebfa995f5ea6f4a29c3f\n",
      "  Building wheel for wildlife-tools (pyproject.toml): started\n",
      "  Building wheel for wildlife-tools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wildlife-tools: filename=wildlife_tools-1.0.1-py3-none-any.whl size=34151 sha256=08b7e6ca9c7122c4ae412ae2657c5d5fb3aa350e4f266bd9cab48aedc824bdc3\n",
      "  Stored in directory: C:\\Users\\trade\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7awgsrz7\\wheels\\8d\\c5\\05\\047fdd4a52c29b09d212c66479d46a674267215340382c8784\n",
      "  Building wheel for lightglue (pyproject.toml): started\n",
      "  Building wheel for lightglue (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for lightglue: filename=lightglue-0.0-py3-none-any.whl size=39803 sha256=834d4c09afdcee467b19e8ac09867f4b70c22ae8d68de9f12ac48e6df6281978\n",
      "  Stored in directory: C:\\Users\\trade\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7awgsrz7\\wheels\\dc\\16\\88\\ad4ddb490c3a6ff37eeface44b776cb588b19e169210e0fbf1\n",
      "Successfully built wildlife-datasets wildlife-tools lightglue\n",
      "Installing collected packages: lightglue, datasets, wildlife-datasets, gluefactory, wildlife-tools\n",
      "Successfully installed datasets-3.4.1 gluefactory-0.0 lightglue-0.0 wildlife-datasets-1.0.5 wildlife-tools-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/WildlifeDatasets/wildlife-datasets 'C:\\Users\\trade\\AppData\\Local\\Temp\\pip-req-build-hek_ycds'\n",
      "  Running command git checkout -b develop --track origin/develop\n",
      "  branch 'develop' set up to track 'origin/develop'.\n",
      "  Switched to a new branch 'develop'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/WildlifeDatasets/wildlife-tools 'C:\\Users\\trade\\AppData\\Local\\Temp\\pip-req-build-fs537d76'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/cvg/LightGlue.git 'C:\\Users\\trade\\AppData\\Local\\Temp\\pip-install-cla5r4lq\\lightglue_b77de58a12fa4c948e988847e28e617c'\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "!pip install git+https://github.com/WildlifeDatasets/wildlife-datasets@develop git+https://github.com/WildlifeDatasets/wildlife-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:20:56.195985Z",
     "iopub.status.busy": "2025-03-11T09:20:56.195594Z",
     "iopub.status.idle": "2025-03-11T09:20:56.201699Z",
     "shell.execute_reply": "2025-03-11T09:20:56.200678Z",
     "shell.execute_reply.started": "2025-03-11T09:20:56.19594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Hub\\DAT550-Animal-CLEF\\venv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Hub\\DAT550-Animal-CLEF\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"VIRTUAL_ENV\"))\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from wildlife_datasets.datasets import WildlifeReID10k\n",
    "from wildlife_datasets.splits import analyze_split\n",
    "from wildlife_datasets.metrics import BAKS, BAUS\n",
    "from utils_wildlifereid10k.utils_wildlifereid10k import get_summary_species, compute_predictions, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install kagglehub\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"wildlifedatasets/wildlifereid-10k\")\n",
    "# path2 = kagglehub.dataset_download(\"wildlifedatasets/wildlifereid-10k-features\")\n",
    "# print(\"Path to dataset files:\", path)\n",
    "#print(\"Path to feature files:\", path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the dataset and show the dataframe with 140,488 images, where each depicts an individual animal. There are 10,772 individual animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:19:43.946064Z",
     "iopub.status.busy": "2025-03-11T09:19:43.945295Z",
     "iopub.status.idle": "2025-03-11T09:19:44.669138Z",
     "shell.execute_reply": "2025-03-11T09:19:44.668018Z",
     "shell.execute_reply.started": "2025-03-11T09:19:43.946014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>identity</th>\n",
       "      <th>path</th>\n",
       "      <th>date</th>\n",
       "      <th>orientation</th>\n",
       "      <th>species</th>\n",
       "      <th>split</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAUZebraFish_5</td>\n",
       "      <td>images/AAUZebraFish/data/Vid2_0520_0000ef3f3f3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "      <td>fish</td>\n",
       "      <td>train</td>\n",
       "      <td>AAUZebraFish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAUZebraFish_3</td>\n",
       "      <td>images/AAUZebraFish/data/Vid1_0062_0002c4ff6de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "      <td>fish</td>\n",
       "      <td>train</td>\n",
       "      <td>AAUZebraFish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAUZebraFish_4</td>\n",
       "      <td>images/AAUZebraFish/data/Vid2_1065_000444965a5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "      <td>fish</td>\n",
       "      <td>train</td>\n",
       "      <td>AAUZebraFish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAUZebraFish_6</td>\n",
       "      <td>images/AAUZebraFish/data/Vid2_0381_001446b650f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "      <td>fish</td>\n",
       "      <td>train</td>\n",
       "      <td>AAUZebraFish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAUZebraFish_2</td>\n",
       "      <td>images/AAUZebraFish/data/Vid1_0582_00173b45939...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left</td>\n",
       "      <td>fish</td>\n",
       "      <td>test</td>\n",
       "      <td>AAUZebraFish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140483</th>\n",
       "      <td>140483</td>\n",
       "      <td>ZindiTurtleRecall_t_id_ip3jsrYo</td>\n",
       "      <td>images/ZindiTurtleRecall/images/ID_ZYTRP3VN_ID...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left</td>\n",
       "      <td>sea turtle</td>\n",
       "      <td>train</td>\n",
       "      <td>ZindiTurtleRecall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140484</th>\n",
       "      <td>140484</td>\n",
       "      <td>ZindiTurtleRecall_t_id_o8HFaaCp</td>\n",
       "      <td>images/ZindiTurtleRecall/images/ID_ZZ04P34G_ID...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sea turtle</td>\n",
       "      <td>test</td>\n",
       "      <td>ZindiTurtleRecall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140485</th>\n",
       "      <td>140485</td>\n",
       "      <td>ZindiTurtleRecall_t_id_ruF8Nbxs</td>\n",
       "      <td>images/ZindiTurtleRecall/images/ID_ZZD2VBPA_ID...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sea turtle</td>\n",
       "      <td>train</td>\n",
       "      <td>ZindiTurtleRecall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140486</th>\n",
       "      <td>140486</td>\n",
       "      <td>ZindiTurtleRecall_t_id_m2JvEcsg</td>\n",
       "      <td>images/ZindiTurtleRecall/images/ID_ZZEGHRM5_ID...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left</td>\n",
       "      <td>sea turtle</td>\n",
       "      <td>train</td>\n",
       "      <td>ZindiTurtleRecall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140487</th>\n",
       "      <td>140487</td>\n",
       "      <td>ZindiTurtleRecall_t_id_mr3e5P1U</td>\n",
       "      <td>images/ZindiTurtleRecall/images/ID_ZZQZJBRE_ID...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sea turtle</td>\n",
       "      <td>test</td>\n",
       "      <td>ZindiTurtleRecall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140488 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id                         identity  \\\n",
       "0              0                   AAUZebraFish_5   \n",
       "1              1                   AAUZebraFish_3   \n",
       "2              2                   AAUZebraFish_4   \n",
       "3              3                   AAUZebraFish_6   \n",
       "4              4                   AAUZebraFish_2   \n",
       "...          ...                              ...   \n",
       "140483    140483  ZindiTurtleRecall_t_id_ip3jsrYo   \n",
       "140484    140484  ZindiTurtleRecall_t_id_o8HFaaCp   \n",
       "140485    140485  ZindiTurtleRecall_t_id_ruF8Nbxs   \n",
       "140486    140486  ZindiTurtleRecall_t_id_m2JvEcsg   \n",
       "140487    140487  ZindiTurtleRecall_t_id_mr3e5P1U   \n",
       "\n",
       "                                                     path date orientation  \\\n",
       "0       images/AAUZebraFish/data/Vid2_0520_0000ef3f3f3...  NaN       right   \n",
       "1       images/AAUZebraFish/data/Vid1_0062_0002c4ff6de...  NaN       right   \n",
       "2       images/AAUZebraFish/data/Vid2_1065_000444965a5...  NaN       right   \n",
       "3       images/AAUZebraFish/data/Vid2_0381_001446b650f...  NaN       right   \n",
       "4       images/AAUZebraFish/data/Vid1_0582_00173b45939...  NaN        left   \n",
       "...                                                   ...  ...         ...   \n",
       "140483  images/ZindiTurtleRecall/images/ID_ZYTRP3VN_ID...  NaN        left   \n",
       "140484  images/ZindiTurtleRecall/images/ID_ZZ04P34G_ID...  NaN         NaN   \n",
       "140485  images/ZindiTurtleRecall/images/ID_ZZD2VBPA_ID...  NaN         NaN   \n",
       "140486  images/ZindiTurtleRecall/images/ID_ZZEGHRM5_ID...  NaN        left   \n",
       "140487  images/ZindiTurtleRecall/images/ID_ZZQZJBRE_ID...  NaN         NaN   \n",
       "\n",
       "           species  split            dataset  \n",
       "0             fish  train       AAUZebraFish  \n",
       "1             fish  train       AAUZebraFish  \n",
       "2             fish  train       AAUZebraFish  \n",
       "3             fish  train       AAUZebraFish  \n",
       "4             fish   test       AAUZebraFish  \n",
       "...            ...    ...                ...  \n",
       "140483  sea turtle  train  ZindiTurtleRecall  \n",
       "140484  sea turtle   test  ZindiTurtleRecall  \n",
       "140485  sea turtle  train  ZindiTurtleRecall  \n",
       "140486  sea turtle  train  ZindiTurtleRecall  \n",
       "140487  sea turtle   test  ZindiTurtleRecall  \n",
       "\n",
       "[140488 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_data = 'C:/Users/trade/.cache/kagglehub/datasets/wildlifedatasets/wildlifereid-10k/versions/6'\n",
    "dataset = WildlifeReID10k(root_data, check_files=False)\n",
    "metadata = dataset.metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a 3*4 sample of the dataset. It is clear that the depicted species, the image quality and the time conditions are extremely different for all images. The sample contains three underwater photos and one night photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:20:12.195677Z",
     "iopub.status.busy": "2025-03-11T09:20:12.195305Z",
     "iopub.status.idle": "2025-03-11T09:20:13.437342Z",
     "shell.execute_reply": "2025-03-11T09:20:13.436147Z",
     "shell.execute_reply.started": "2025-03-11T09:20:12.19565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.plot_grid(n_rows=3, n_cols=4, idx=np.arange(0, len(dataset), 10000)[::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show summary of the incorporated species. The majority of species are wild and domestic species are in minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:20:30.102192Z",
     "iopub.status.busy": "2025-03-11T09:20:30.101645Z",
     "iopub.status.idle": "2025-03-11T09:20:30.178515Z",
     "shell.execute_reply": "2025-03-11T09:20:30.17628Z",
     "shell.execute_reply.started": "2025-03-11T09:20:30.10215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_summary_species(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits for algorithm training\n",
    "\n",
    "WildlifeReID-10k contains a default split. The split is open-set, meaning that some animals are only in the testing set but not in the training set. For such animals, the algorithm should predict that they are new. The evaluation is possible for both the open-set and closed-set, where for the latter the individuals only in the testing set are ignored. The following summary shows that the training set consists of 79.65% images, the new individuals 10.68% and the known individuals the remaining 9.67%. In other words, the open-set problem will be evaluated at 20.35% of the dataset, while the closed-set only at 9.67% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:21:03.103517Z",
     "iopub.status.busy": "2025-03-11T09:21:03.103154Z",
     "iopub.status.idle": "2025-03-11T09:22:01.11451Z",
     "shell.execute_reply": "2025-03-11T09:22:01.113307Z",
     "shell.execute_reply.started": "2025-03-11T09:21:03.10349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "idx_train = np.where(metadata['split'] == 'train')[0]\n",
    "idx_test = np.where(metadata['split'] == 'test')[0]\n",
    "analyze_split(metadata, idx_train, idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indeed verify that there are 946 individuals which are in the testing set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:22:01.116695Z",
     "iopub.status.busy": "2025-03-11T09:22:01.116366Z",
     "iopub.status.idle": "2025-03-11T09:22:01.154788Z",
     "shell.execute_reply": "2025-03-11T09:22:01.153772Z",
     "shell.execute_reply.started": "2025-03-11T09:22:01.116666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "identity = metadata['identity'].to_numpy()\n",
    "identity_train = identity[metadata['split'] == 'train']\n",
    "identity_test = identity[metadata['split'] == 'test']\n",
    "identity_test_only = list(set(identity_test) - set(identity_train))\n",
    "len(identity_test_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance\n",
    "\n",
    "We assume that the user has already trained some of his algorithm on the training set. We have used [MiewID](https://huggingface.co/conservationxlabs/miewid-msv3) to extract features. It is also possible to use features extracted by [MegaDescriptor](https://huggingface.co/BVRA/MegaDescriptor-L-384).\n",
    "\n",
    "The predictions are computed dataset-wise. We therefore make a loop over datasets. In each loop, we load the corresponding features and make the predictions based on 1-NN with similarity scores. The similarity score is the cosine similarity, measuring the angle between the features. Whenever the similarity score is below the threshold `t`, we decide that the prediction is not sufficiently strong and predict `new_individual` instead. We compute the BAKS (balanced accuracy on known samples) and BAUS (balanced accuracy on unknown samples) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:22:21.543089Z",
     "iopub.status.busy": "2025-03-11T09:22:21.542319Z",
     "iopub.status.idle": "2025-03-11T09:30:13.33104Z",
     "shell.execute_reply": "2025-03-11T09:30:13.329853Z",
     "shell.execute_reply.started": "2025-03-11T09:22:21.543048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# root_features = '/kaggle/input/wildlifereid-10k-features/features_miew'\n",
    "#root_features = '/kaggle/input/wildlifereid-10k-features/features_mega'\n",
    "root_features = 'C:/Users/trade/.cache/kagglehub/datasets/wildlifedatasets/wildlifereid-10k-features/versions/2'\n",
    "\n",
    "step = 0.01\n",
    "ts = [-np.inf] + list(np.round(np.arange(0, 1+step/10, step), 2)) + [np.inf]\n",
    "new_individual = 'new_individual'\n",
    "\n",
    "baks = {t: {} for t in ts}\n",
    "baus = {t: {} for t in ts}\n",
    "result = {}\n",
    "for name, metadata_dataset in dataset.metadata.groupby('dataset'):\n",
    "    print(name)\n",
    "    features = np.load(f'{root_features}/features_{name}.npy')\n",
    "\n",
    "    idx_train = np.where(metadata_dataset['split'] == 'train')[0]\n",
    "    idx_test = np.where(metadata_dataset['split'] == 'test')[0]\n",
    "\n",
    "    idx_true, idx_pred, similarity = compute_predictions(features[idx_test], features[idx_train], return_score=True)\n",
    "    idx_true = idx_test[idx_true]\n",
    "    idx_pred = idx_train[idx_pred]\n",
    "    idx_pred = idx_pred[:,0]\n",
    "    similarity = similarity[:,0]\n",
    "\n",
    "    y_true = metadata_dataset['identity'].iloc[idx_true]\n",
    "    y_pred_closed = metadata_dataset['identity'].iloc[idx_pred]\n",
    "\n",
    "    identity_test_only = list(set(metadata_dataset['identity'].iloc[idx_test]) - set(metadata_dataset['identity'].iloc[idx_train]))\n",
    "\n",
    "    for t in ts:\n",
    "        y_pred = copy.copy(y_pred_closed)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            y_pred[similarity < t] = new_individual\n",
    "        baks[t][name] = BAKS(y_true, y_pred, identity_test_only)\n",
    "        baus[t][name] = BAUS(y_true, y_pred, identity_test_only, new_individual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot BAKS and BAUS averaged over datasets for different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:41:05.316817Z",
     "iopub.status.busy": "2025-03-11T09:41:05.316426Z",
     "iopub.status.idle": "2025-03-11T09:41:05.537257Z",
     "shell.execute_reply": "2025-03-11T09:41:05.536156Z",
     "shell.execute_reply.started": "2025-03-11T09:41:05.316785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_baks = np.array([mean(baks[t]) for t in ts])\n",
    "data_baus = np.array([mean(baus[t]) for t in ts])\n",
    "\n",
    "plt.scatter(data_baks, data_baus, color='blue', marker='+')\n",
    "plt.xlabel('BAKS')\n",
    "plt.ylabel('BAUS')\n",
    "plt.xlim([0, 1.01])\n",
    "plt.ylim([0, 1.01]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity, we plot the normalized accuracy, which is the geometric mean between BAKS and BAUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:41:19.723645Z",
     "iopub.status.busy": "2025-03-11T09:41:19.723287Z",
     "iopub.status.idle": "2025-03-11T09:41:19.936821Z",
     "shell.execute_reply": "2025-03-11T09:41:19.935768Z",
     "shell.execute_reply.started": "2025-03-11T09:41:19.723619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ts, np.sqrt(data_baks * data_baus))\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('normalized accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the baseline results. For the closed-set, we select the threshold $t=-\\infty$, where no predictions are made as new individual. Due to the structure of BAKS, all new individuals from the testing set are ignored during inference. For the open-set, we arbitrarily select the threshold $t=0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T09:43:09.589699Z",
     "iopub.status.busy": "2025-03-11T09:43:09.589016Z",
     "iopub.status.idle": "2025-03-11T09:43:09.595029Z",
     "shell.execute_reply": "2025-03-11T09:43:09.594007Z",
     "shell.execute_reply.started": "2025-03-11T09:43:09.589666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'Closed-set accuracy = {mean(baks[-np.inf])}')\n",
    "print(f'Open-set normalized accuracy = {np.sqrt(mean(baks[0.7])*mean(baus[0.7]))}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4867966,
     "sourceId": 10953918,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5198777,
     "sourceId": 10983490,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 183027933,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
